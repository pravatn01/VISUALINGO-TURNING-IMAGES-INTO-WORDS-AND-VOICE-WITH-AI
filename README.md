# VISUALINGO ğŸ–¼ï¸â¡ï¸ğŸ“ğŸ”Š  
**Turn images into captionsâ€”and voiceâ€”with AI.**

VISUALINGO is an AI-powered image captioning system that lets users generate image descriptions and hear them spoken aloud. It compares the performance of two distinct models:

- **CNN (DenseNet201) for feature extraction + LSTM decoder**, trained on the **Flickr8k** dataset using **TensorFlow**
- **Pretrained BLIP Transformer**, from Hugging Face (requires **PyTorch** runtime)

The app includes a user-friendly **Gradio UI**, where users can upload images, view captions generated by both models, and listen to them via **Google Text-to-Speech (gTTS)**.

---

## ğŸ§  Model Overview

| Model        | Architecture                        | Dataset   | Framework      |
|--------------|-------------------------------------|-----------|----------------|
| Baseline     | DenseNet201 (features) + LSTM       | Flickr8k  | TensorFlow     |
| Transformer  | BLIP (Hugging Face, pretrained)     | N/A       | PyTorch Runtime|

---

## ğŸš€ Run Locally

### Step 1: Clone the Repository

```bash
git clone https://github.com/pravatn01/Visualingo-Images-to-Words-and-Voice-with-AI.git
cd Visualingo-Images-to-Words-and-Voice-with-AI
```

### Step 2: Create and Activate Conda Environment

Make sure you have [Anaconda](https://www.anaconda.com/download/success) installed.

```bash
conda env create -f environment.yml
conda activate visualingo
```

### Step 3: Launch the Web App

```bash
python webapp.py
```

---

## ğŸ›  Tech Stack

- **TensorFlow** â€“ for training and running the CNN + LSTM model  
- **PyTorch (runtime only)** â€“ required for the pretrained BLIP Transformer  
- **Gradio** â€“ web UI for image upload, caption display, and audio playback  
- **gTTS** â€“ converts text captions to speech  
- **Flickr8k** â€“ dataset used to train the baseline model  
- **Matplotlib** â€“ used for visualizing training/validation loss curves
